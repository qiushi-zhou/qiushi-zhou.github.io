---
layout: page
title: Recent Publications
permalink: /publications/
---
<img src="/assets/images/hololens.png"
     alt=""
     />

**INTERACT 2023** <a href="https://qiushi-zhou.github.io/PDF/INTERACT-2023-Weibo.pdf">PDF</a>
## “Hello, Fellow Villager!”: Perceptions and Impact of Displaying Users’ Locations on Weibo

_Ying Ma, **Qiushi Zhou**, Benjamin Tag, Zhanna Sarsenbayeva, Jarrod Knibbe, Jorge Goncalves_

In April 2022, Sina Weibo began to display users' coarse location for the stated purpose of regulating their online community. However, this raised concerns about location privacy. Through sentiment analysis and Latent Dirichlet Allocation (LDA), we analysed the users' attitudes and opinions on this topic across 20,162 related posts and comments. We labelled 300 users as either supportive, critical, or neutral towards the feature, and captured their posting behaviour two months prior and two months post the launch. Our analysis elicits three major themes in the public discussion: online community atmosphere, privacy issues, and equity in the application of the feature, and shows that most people expressed a negative attitude. We find a drop in activity by objectors during the first month after the launch of the feature before gradually resuming. This work provides a large-scale firsthand account of people's attitudes and opinions towards online location privacy on social media platforms.

<div class="divider"></div>

**CHI 2023** <a href="https://qiushi-zhou.github.io/PDF/CHI-2023-Dance.pdf">PDF</a>
## Here and Now: Creating Improvisational Dance Movements with a Mixed Reality Mirror

_**Qiushi Zhou**, Louise Grebel, Andrew Irlitti, Julie Ann Minaai,  Jorge Goncalves, Eduardo Velloso_

This paper explores using mixed reality (MR) mirrors for supporting improvisational dance making. Motivated by the prevalence of mirrors in dance studios and inspired by Forsythe’s Improvisation Technologies, we conducted workshops with 13 dancers and choreographers to inform the design of future MR visualisation and annotation tools for dance. The workshops involved using a prototype MR mirror as a technology probe that reveals the spatial and temporal relationships between the reflected dancing body and its surroundings during improvisation; speed dating group interviews around future design ideas; follow-up surveys and extended interviews with a digital media dance artist and a dance educator. Our findings highlight how the MR mirror enriches dancers’ temporal and spatial perception, creates multi-layered presence, and affords appropriation by dancers. We also discuss the unique place of MR mirrors in the theoretical context of dance and in the history of movement visualisation, and distil lessons for broader HCI research.

<div class="divider"></div>

**CHI 2023** <a href="https://qiushi-zhou.github.io/">PDF</a>
## Volumetric Mixed Reality Telepresence for Real-time Cross Modality Collaboration

_Andrew Irlitti, Mesut Latifoglu, **Qiushi Zhou**, Martin Reinoso, Eduardo Velloso, Thuong Hoang, Frank Vetere_

Mixed-reality telepresence allows local and remote users feel as if they are present together in the same space. In this paper we report on a mixed-reality volumetric telepresence system that is adaptable, multi-user and cross-modal, i.e. combining augmented and virtual reality technologies with face-to-face interactions. The system extends state-of-art by creating full-body and environmental volumetric renderings in real-time over local enterprise networks. We report findings of an evaluation in a training scenario which was adapted for remote delivery and led by an industry professional. Analysis of interviews and observed behaviours identify varying attitudes towards virtually mediated full-body experiences and highlight the impact of volumetric mixed-reality telepresence to facilitate personal experiences of co-presence and to ground communication with interlocutors.

<div class="divider"></div>

**ISMAR 2022** <a href="https://qiushi-zhou.github.io/PDF/ISMAR-2022-Body.pdf">PDF</a>
## Blending On-Body and Mid-Air Interaction in Virtual Reality

_Difeng Yu, **Qiushi Zhou**, Tilman Dingler, Eduardo Velloso, Jorge Goncalves_

On-body interfaces, which leverage the human body’s surface as an input or output platform, can provide new opportunities for designing VR interaction. However, it remains unclear how on-body interfaces can best support current VR systems that mainly rely on mid-air interaction. We propose BodyOn, a collection of six design patterns that leverage combined on-body and mid-air interfaces to achieve more effective 3D interaction. Specifically, a user may use thumb-on-finger gestures, finger-on-arm gestures, or on-body displays with mid-air input, including hand movement and orientation, to complete an interaction task. To test our design concepts, we implemented example interaction techniques based on BodyOn that can assist users in various 3D interaction tasks. We further conducted an expert evaluation using the techniques as probes to elicit immediate design issues that emerge from the novel combination of on-body and midair interaction. We provide insights that can inspire and inform the design of future 3D user interfaces.

<div class="divider"></div>

**DIS 2022** <a href="https://qiushi-zhou.github.io/PDF/DIS-2022-Movement.pdf">PDF</a>
## Movement Guidance using a Mixed Reality Mirror

**_Qiushi Zhou_**_, Andrew Irlitti, Difeng Yu, Jorge Goncalves, Eduardo Velloso_

Mirror reflections offer an intuitive and realistic Mixed Reality (MR) experience comparable to other MR interfaces. Their high visual fidelity, and the sensorimotor contingency from the reflected moving body, make the mirror an ideal instrument for MR movement guidance. The translucent two-way mirror display enables users to follow a virtual humanoid instructor's movement accurately by visually matching it with their reflections. In this work, we conduct the first formal evaluation of movement acquisition performance with simple motor tasks, using visual guidance from an MR mirror and a humanoid virtual instructor. Our results of performance and subjective ratings indicate that, comparing with simulated virtual mirror and with traditional screen-based movement guidance, the real MR mirror yields better acquisition performance and stronger sense of embodiment with the reflection, for upper-body movement. But the benefits diminish with larger-range head movements. We provide design guidelines for future mirror movement guidance interfaces and MR mirror experiences at large.

<div class="divider"></div>

**&#127941; CHI 2021 (Honourable Mention)** <a href="https://qiushi-zhou.github.io/PDF/CHI-2021-dance.pdf">PDF</a>
## Dance and Choreography in HCI: A Two-Decade Retrospective

**_Qiushi Zhou_**_, Chengcheng Chua, Jarrod Knibbe, Jorge Goncalves, Eduardo Velloso_

Designing computational support for dance is an emerging area of HCI research, incorporating the cultural, experiential, and embodied characteristics of the third-wave shift. The challenges of recognising the abstract qualities of body movement, and of mediating between the diverse parties involved in the idiosyncratic creative process, present important questions to HCI researchers: how can we effectively integrate computing with dance, to understand and cultivate the felt dimension of creativity, and to aid the dance-making process? In this work, we systematically review the past twenty years of dance literature in HCI. We discuss our findings, propose directions for future HCI works in dance, and distil lessons for related disciplines. 

<div class="divider"></div>

**IEEE TVCG 2020 (ISMAR)** <a href="https://qiushi-zhou.github.io/PDF/TVCG-2020-Eyes.pdf">PDF</a>
## Eyes-free Target Acquisition During Walking in Immersive Mixed Reality

_**Qiushi Zhou**, Difeng Yu, Martin Reinoso, Joshua Newn, Jorge Goncalves, Eduardo Velloso_

Reaching towards out-of-sight objects during walking is a common task in daily life, however the same task can be challenging when wearing immersive Head-Mounted Displays (HMD). In this paper, we investigate the effects of spatial reference frame, walking path curvature, and target placement relative to the body on user performance of manually acquiring out-of-sight targets located around their bodies, as they walk in a spatial-mapping Mixed Reality (MR) environment wearing an immersive HMD. We found that walking and increased path curvature negatively affected the overall spatial accuracy of the performance, and that the performance benefited more from using the torso as the reference frame than the head. We also found that targets placed at maximum reaching distance yielded less error in angular rotation and depth of the reaching arm. We discuss our findings with regard to human walking kinesthetics and the sensory integration in the peripersonal space during locomotion in immersive MR. We provide design guidelines for future immersive MR experience featuring spatial mapping and full-body motion tracking to provide better embodied experience. 

<div class="divider"></div>

**&#127941; IEEE TVCG 2020 (ISMAR Best Paper Nomination)** <a href="https://qiushi-zhou.github.io/PDF/TVCG-2020-Fully.pdf">PDF</a>
## Fully-Occluded Target Selection in Virtual Reality

_Difeng Yu, **Qiushi Zhou**, Joshua Newn, Tilman Dingler, Jorge Goncalves, Eduardo Velloso_

The presence of fully-occluded targets is common within virtual environments, ranging from a virtual object located behind a wall to a datapoint of interest hidden in a complex visualization. However, efficient input techniques for locating and selecting these targets are mostly underexplored in virtual reality (VR) systems. In this paper, we developed an initial set of seven techniques techniques for fully-occluded target selection in VR. We then evaluated their performance in a user study and derived a set of design implications for simple and more complex tasks from our results. Based on these insights, we refined the most promising techniques and conducted a second, more comprehensive user study. Our results show how factors, such as occlusion layers, target depths, object densities, and the estimation of target locations, can affect technique performance. Our findings from both studies and distilled recommendations can inform the design of future VR systems that offer selections for fully-occluded targets. 

<div class="divider"></div>

**CHI 2020** <a href="https://qiushi-zhou.github.io/PDF/CHI-2020-Faces.pdf">PDF</a>
## Faces of Focus: A Study on the Facial Cues of Attentional States

_Ebrahim Babaei, Namrata Srivastava, Joshua Newn, **Qiushi Zhou**, Tilman Dingler, Eduardo Velloso_

Automatically detecting attentional states is a prerequisite for designing interventions to manage attention---knowledge workers' most critical resource. As a first step towards this goal, it is necessary to understand how different attentional states are made discernible through visible cues in knowledge workers. In this paper, we demonstrate the important facial cues to detect attentional states by evaluating a data set of 15 participants that we tracked over a whole workday, which included their challenge and engagement levels. Our evaluation shows that gaze, pitch, and lips part action units are indicators of engaged work; while pitch, gaze movements, gaze angle, and upper-lid raiser action units are indicators of challenging work. These findings reveal a significant relationship between facial cues and both engagement and challenge levels experienced by our tracked participants. Our work contributes to the design of future studies to detect attentional states based on facial cues. 

<div class="divider"></div>

**IEEE VR 2020** <a href="https://qiushi-zhou.github.io/PDF/VR-2020-Engaging.pdf">PDF</a>
## Engaging Participants during Selection Studies in Virtual Reality

_Difeng Yu, **Qiushi Zhou**, Benjamin Tag, Tilman Dingler, Eduardo Velloso, Jorge Goncalves_

Selection studies are prevalent and indispensable for VR research. However, due to the tedious and repetitive nature of many such experiments, participants can become disengaged during the study, which is likely to impact the results and conclusions. In this work, we investigate participant disengagement in VR selection experiments and how this issue affects the outcomes. Moreover, we evaluate the usefulness of four engagement strategies to keep participants engaged during VR selection studies and investigate how they impact user performance when compared to a baseline condition with no engagement strategy. Based on our findings, we distill several design recommendations that can be useful for future VR selection studies or user tests in other domains that employ similar repetitive features.

<div class="divider"></div>

**IMWUT 2019 (UbiComp Workshop)** <a href="https://qiushi-zhou.github.io/PDF/IMWUT-2019-Ubiquitous.pdf">PDF</a>
## Ubiquitous Smart Eyewear Interactions using Implicit Sensing and Unobtrusive Information Output

_**Qiushi Zhou**, Joshua Newn, Benjamin Tag, Hao-Ping Lee, Chaofan Wang, Eduardo Velloso_

Premature technology, privacy, intrusiveness, power consumption, and user habits are all factors potentially contributing to the lack of social acceptance of smart glasses. After investigating the recent development of commercial smart eyewear and its related research, we propose a design space for ubiquitous smart eyewear interactions while maximising interactivity with minimal obtrusiveness. We focus on implicit and explicit interactions enabled by the combination of miniature sensor technology, low-resolution display and simplistic interaction modalities. Additionally, we are presenting example applications outlining future development directions. Finally, we aim at raising the awareness of designing for ubiquitous eyewear with implicit sensing and unobtrusive information output abilities.

<div class="divider"></div>

**CHI 2019 (LBW)** <a href="https://qiushi-zhou.github.io/PDF/CHI-2019-Cognitive.pdf">PDF</a>
## Cognitive Aid: Task Assistance Based On Mental Workload Estimation

_**Qiushi Zhou**, Joshua Newn, Namrata Srivastava, Tilman Dingler, Jorge Goncalves, Eduardo Velloso_

In this work, we evaluate the potential of using wearable non-contact (infrared) thermal sensors through a user study (N=12) to measure mental workload. Our results indicate the possibility of mental workload estimation through the temperature changes detected using the prototype as participants perform two task variants with increasing difficulty levels. While the sensor accuracy and the design of the prototype can be further improved, the prototype showed the potential of building AR-based systems with cognitive aid technology for ubiquitous task assistance from the changes in mental workload demands. As such, we demonstrate our next steps by integrating our prototype into an existing AR headset (i.e.~Microsoft HoloLens).

<div class="divider"></div>

**&#127941; OzCHI 2017 (Honourable Mention)** <a href="https://qiushi-zhou.github.io/PDF/OZCHI-2017-Gazegrip.pdf">PDF</a>
## GazeGrip: Improving Mobile Device Accessibility with Gaze & Grip Interaction

_**Qiushi Zhou**, Eduardo Velloso_

Though modern tablet devices offer users high processing power in a compact form factor, interaction while holding them still presents problems, forcing the user to alternate the dominant hand between holding and touching the screen. In this paper, we explore how eye tracking can minimize this problem through GazeGrip---a prototype interactive system for a tablet that integrates eye tracking and back-of-device touch sensing. We propose a design space for potential interaction techniques that leverage the power of this combination, as well as prototype applications that instantiate it. Our preliminary results highlight as opportunities enabled by the system reduced fatigue while holding the device, minimal occlusion of the screen, and improved accuracy and precision in the interaction.

<div class="divider"></div>